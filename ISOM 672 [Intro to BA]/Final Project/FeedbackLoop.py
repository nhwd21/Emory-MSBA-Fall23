# -*- coding: utf-8 -*-
"""FinalProjectIntroToBA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LnMeQyUoCtkbpzCq8FkhT3bedtq8Wtr_

Step 1. Load Data
"""

#read in_time.csv and out_time.csv
import pandas as pd
in_time = pd.read_csv('C:\\Users\\d_mos\\OneDrive\\Documents\\ISOM 672 [Intro to BA]\\Final Project\\in_time.csv')
out_time = pd.read_csv('C:\\Users\\d_mos\\OneDrive\\Documents\\ISOM 672 [Intro to BA]\\Final Project\\out_time.csv')

#rename first column to 'id'
in_time.rename(columns={'Unnamed: 0':'id'}, inplace=True)
in_time.iloc[:,1:] = in_time.iloc[:,1:].apply(pd.to_datetime)
in_time.head()

#rename first column to 'id'
out_time.rename(columns={'Unnamed: 0':'id'}, inplace=True)
#convert all columns but 'id' to datetime
out_time.iloc[:,1:] = out_time.iloc[:,1:].apply(pd.to_datetime)
out_time.head()

# create a df of the time difference between all columns and rows with matching indexes in_time and out_time
time_diff = pd.DataFrame(out_time.values - in_time.values, columns=in_time.columns, index=in_time.index)
time_diff['id'] = in_time['id']
time_diff.head()

#convert all columns but 'id' from datetime to number of hours
for i in range(1, len(time_diff.columns)):
    time_diff.iloc[:,i] = time_diff.iloc[:,i].apply(lambda x: x.total_seconds()/3600)

time_diff.head()

#averge the time difference for each id
time_diff['avg_time'] = time_diff.iloc[:,1:].mean(axis=1)
time_diff[['avg_time', 'id']].to_csv('C:\\Users\\d_mos\\OneDrive\\Documents\\ISOM 672 [Intro to BA]\\Final Project\\avg_time_diff.csv', index=False)
time_diff[['avg_time', 'id']].head(10)

actual_work_time = time_diff[['avg_time', 'id']]

#load general_data
general_data = pd.read_csv('C:\\Users\\d_mos\\OneDrive\\Documents\\ISOM 672 [Intro to BA]\\Final Project\\general_data.csv')

#creating 'id'
general_data['id'] = general_data.index + 1

# Reorder the columns to make 'row_id' the first column
general_data = general_data[['id'] + [col for col in general_data.columns if col != 'id']]

general_data.head()

#join table
df = pd.merge(general_data, actual_work_time, on='id', how='left')

"""### encoding categorical variables"""

# binary gender directly replace
df['Gender'].value_counts()

df['Gender'] = df['Gender'].replace({'Male': 1, 'Female': 0})
df['Over18'] = df['Over18'].replace({'Y': 1, 'N': 0})

df['BusinessTravel'].value_counts()

df['MaritalStatus'].value_counts()

df['Department'].value_counts()

df['EducationField'].value_counts()

df['JobRole'].value_counts()

"""'BusinessTravel' is ordinal so use Label Encoding: Convert each category to a unique integer. This approach is suitable for ordinal categorical variables where there is an inherent order

'Department', 'JobRole', 'MaritalStatus' and 'EducationField' are not ordinal so use One-Hot Encoding: Convert each categorical feature into binary (0 or 1) columns for each category. This approach is suitable when there is no ordinal relationship between categories.

*We'll use dept='human resources', jobrole= 'Manager', MaritalStatus = 'single' and edu='other' category as reference level*
"""

df['BusinessTravel'] = df['BusinessTravel'].replace({'Non-Travel': 1, 'Travel_Rarely': 2,'Travel_Frequently':3})

from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
encoded_data = encoder.fit_transform(df[['Department']]) #One-hot encoding converts categorical data into a format that can be provided to machine learning algorithms.

pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out(['Department']))

encoder2 = OneHotEncoder()
encoded_data2 = encoder2.fit_transform(df[['EducationField']])
encoder3 = OneHotEncoder()
encoded_data3 = encoder3.fit_transform(df[['MaritalStatus']])
encoder4 = OneHotEncoder()
encoded_data4 = encoder4.fit_transform(df[['JobRole']])

df = pd.concat((df, pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out(['Department']))), axis=1)
df = pd.concat((df, pd.DataFrame(encoded_data2.toarray(), columns=encoder2.get_feature_names_out(['EducationField']))), axis=1)
df = pd.concat((df, pd.DataFrame(encoded_data3.toarray(), columns=encoder3.get_feature_names_out(['MaritalStatus']))), axis=1)
df = pd.concat((df, pd.DataFrame(encoded_data4.toarray(), columns=encoder4.get_feature_names_out(['JobRole']))), axis=1)

#drop the columns of reference level for each categorical feature

df.drop('Department_Research & Development',axis=1,inplace = True)
df.drop('EducationField_Other',axis=1,inplace = True)
df.drop('MaritalStatus_Single',axis=1,inplace = True)
df.drop('JobRole_Manager',axis=1,inplace = True)

df.head()

df.shape

df.isna().sum() #19 NAs in NumCompaniesWorked

df.dropna(inplace=True)

"""Outliers"""

# Coninuous variables columns
cont_cols = ['DistanceFromHome', 'MonthlyIncome', 'NumCompaniesWorked', 'TotalWorkingYears',
             'TrainingTimesLastYear', 'YearsAtCompany', 'YearsSinceLastPromotion', 'YearsWithCurrManager']

# Function to drop outliers
def drop_outliers(df, cont_cols):
    for column in cont_cols:
        if column in df.columns:
            Q1 = df[column].quantile(0.25)
            Q3 = df[column].quantile(0.75)
            IQR = Q3 - Q1
            df = df[(df[column] >= (Q1 - 3 * IQR)) & (df[column] <= (Q3 + 3 * IQR))]
    return df

# Drop outliers
df_clean = drop_outliers(df, cont_cols)
print(df_clean)

df.shape

df_clean.shape

# Mapping y to 1 and 0
df_clean['Attrition'] = df_clean['Attrition'].map({'Yes': 1, 'No': 0})

"""# Imbalanced Y handling & train-test-split"""

df_clean['Attrition'].value_counts()
# imbalanced possitive and negative Y value, the possitive attrition = Yes is minority class

from sklearn.model_selection import train_test_split

X = df_clean.drop(['Attrition','EducationField','Department','MaritalStatus','JobRole'], axis=1)
y = df_clean['Attrition']

X_train, X_test0, y_train, y_test0 = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

X_train

"""SMOTE addresses the class imbalance by augmenting the dataset with synthetic instances rather than by oversampling (which can lead to overfitting) or undersampling (which can lead to loss of information). For every instance in the minority class, SMOTE selects 'k' nearest neighbors and generates synthetic instances between the chosen instance and its neighbors. Applying SMOTE:"""

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_train, y_train)

y_smote = y_smote.to_frame()
y_smote['Attrition'].value_counts()

"""#Feature Engineering"""

X_smote

"""Adding new feature Avg_Working_Time"""

# New Feature (Avg_Working_Time - Standard_Hours )/ Standard_Hours %
# Calculate the new feature 'overtime_percentage'
X_smote['overtime_percentage'] = abs(((X_smote['avg_time'] - X_smote['StandardHours']) / X_smote['StandardHours']) * 100)

# Drop the original columns 'Avg_Working_Time' and 'Standard_Hours'
X_smote.drop(['avg_time', 'StandardHours'], axis=1, inplace=True)

# Calculate skewness for each column
skewness_scores = X_smote.skew(numeric_only=True)

# Print or return the skewness scores
print(skewness_scores)

# add 1/3 and take the log of the highly skewed x variables
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# List of highly skewed columns
skewed_columns = ['DistanceFromHome', 'MonthlyIncome', 'NumCompaniesWorked', 'TotalWorkingYears', 'YearsAtCompany', 'YearsSinceLastPromotion','YearsWithCurrManager']

# Create a copy of the df_clean DataFrame
X_smote_log = X_smote.copy()

# Apply the logarithm transformation to the skewed columns with an offset of 1/3 in the new DataFrame
X_smote_log[skewed_columns] = X_smote_log[skewed_columns].apply(lambda x: np.log1p(x + 1/3))

# Create a figure with subplots
fig, axes = plt.subplots(nrows=1, ncols=len(skewed_columns), figsize=(20, 5))

# Iterate through columns and plot histograms from the new DataFrame
for i, column in enumerate(skewed_columns):
    axes[i].hist(X_smote_log[column], bins=20, edgecolor='k')
    axes[i].set_title(column)
    axes[i].set_xlabel('Value')
    axes[i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

# y_smote = y_smote.to_frame()

# repeat all above changes to X_train/X_smote for X_test
X_test0['overtime_percentage'] = abs(((X_test0['avg_time'] - X_test0['StandardHours']) / X_test0['StandardHours']) * 100)
X_test0.drop(['avg_time', 'StandardHours'], axis=1, inplace=True)
X_test0[skewed_columns] = X_test0[skewed_columns].apply(lambda x: np.log1p(x + 1/3))
#y_test
y_test = y_test0.to_frame()

"""Polynomial terms"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import numpy as np

# Dependent variable
y = y_smote['Attrition']

# Continuous independent variables
features = ['Age', 'DistanceFromHome', 'MonthlyIncome', 'NumCompaniesWorked',
            'PercentSalaryHike','TotalWorkingYears',
            'TrainingTimesLastYear', 'YearsAtCompany', 'YearsSinceLastPromotion',
            'YearsWithCurrManager']

# Initialize polynomial feature variables
poly2 = PolynomialFeatures(degree=2)
poly3 = PolynomialFeatures(degree=3)

# Initialize linear regression model
model = LinearRegression()

# Loop through each feature
for feature in features:
    X = X_smote_log[[feature]]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Linear
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2_linear = r2_score(y_test, y_pred)

    # Quadratic
    X_poly2 = poly2.fit_transform(X_train)
    model.fit(X_poly2, y_train)
    y_pred = model.predict(poly2.transform(X_test))
    r2_quad = r2_score(y_test, y_pred)

    # Cubic
    X_poly3 = poly3.fit_transform(X_train)
    model.fit(X_poly3, y_train)
    y_pred = model.predict(poly3.transform(X_test))
    r2_cubic = r2_score(y_test, y_pred)

    print(f"{feature} - R2 linear: {r2_linear}, R2 quadratic: {r2_quad}, R2 cubic: {r2_cubic}")

# Results suggesting Age quadratic

# Add quadratic term for 'Age'
X_smote_log['Age_squared'] = X_smote_log['Age'] ** 2

# repeat all above changes to X_train/X_smote for X_test
X_test0['Age_squared'] = X_test0['Age'] ** 2

"""interaction terms"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import pandas as pd

# Dependent variable
y = y_smote['Attrition']

# Continuous independent variables
features = ['Age', 'DistanceFromHome', 'MonthlyIncome', 'NumCompaniesWorked',
            'PercentSalaryHike', 'TotalWorkingYears',
            'TrainingTimesLastYear', 'YearsAtCompany', 'YearsSinceLastPromotion',
            'YearsWithCurrManager', 'overtime_percentage']

# Initialize linear regression model
model = LinearRegression()

# Loop through each feature
for feature in features:
    X = X_smote_log[[feature]]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2_without_interaction = r2_score(y_test, y_pred)

    # Interaction Terms
    for i in range(len(features)):
        if features[i] != feature:
            interaction_name = f'{feature}_x_{features[i]}'

            # Create a copy of the DataFrame to avoid warnings
            X_interaction = X_smote_log[[feature, features[i]]].copy()
            X_interaction['interaction'] = X_interaction[feature] * X_interaction[features[i]]
            X_train_int, X_test_int, y_train_int, y_test_int = train_test_split(X_interaction, y, test_size=0.2, random_state=42)
            model.fit(X_train_int, y_train_int)
            y_pred_int = model.predict(X_test_int)
            r2_interaction = r2_score(y_test_int, y_pred_int)

            # Check if the interaction term improves the model by at least 12.5%
            if r2_interaction > r2_without_interaction + 0.125:
                print(f"{interaction_name} - R2 interaction: {r2_interaction}. This interaction term is needed.")
            else:
                continue

# Create the interaction terms
X_smote_log['NumCompaniesWorked_x_TotalWorkingYears'] = X_smote_log['NumCompaniesWorked'] * X_smote_log['TotalWorkingYears']
X_smote_log['overtime_percentage_x_YearsAtCompany'] = X_smote_log['overtime_percentage'] * X_smote_log['YearsAtCompany']

X_smote_log

y_smote

# repeat all above changes to X_train/X_smote for X_test

# Create the interaction terms
X_test0['NumCompaniesWorked_x_TotalWorkingYears'] = X_test0['NumCompaniesWorked'] * X_test0['TotalWorkingYears']
X_test0['overtime_percentage_x_YearsAtCompany'] = X_test0['overtime_percentage'] * X_test0['YearsAtCompany']

"""#Feature Selection"""

# Drop index column
X_smote_log_dropped = X_smote_log.drop(['id', 'EmployeeID'], axis=1)

# Drop employee count
X_smote = X_smote_log_dropped.drop(['EmployeeCount'], axis=1)

# repeat all above changes to X_train/X_smote for X_test
X_test_log_dropped = X_test0.drop(['id', 'EmployeeID'], axis=1)
X_test0 = X_test_log_dropped.drop(['EmployeeCount'], axis=1)

X_smote.columns

y_smote.columns

"""#Step 3. Modeling

Make sure to use the correct variables for each modeling step; all variables (transformed and not) are in the df!

## Decision Tree

### Setup
"""

############################### Import Libraries & Modules #################################
from sklearn.tree import DecisionTreeClassifier # A decision tree classifier
# GridSearchCV performs an exhaustive search over specified parameter values for an estimator
# The parameters of the estimator used to apply these methods are optimized by cross-validated
# grid-search over a parameter grid.
# Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
from sklearn.model_selection import GridSearchCV, KFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn import neighbors, datasets
# Standardize features by removing the mean and scaling to unit variance
from sklearn.preprocessing import StandardScaler
# Pipeline of transforms with a final estimator
from sklearn.pipeline import Pipeline


np.random.seed(42) # ensure reproducability

"""### Initiate Cross Validation"""

################################# Nested Cross-Validation #################################

##################################### Parameter Tuning ####################################

# Exhaustive search over specified parameter values for an estimator.
# GridSearchCV implements a “fit” and a “score” method.
# It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform”
# if they are implemented in the estimator used.

# The parameters of the estimator used to apply these methods are optimized by cross-validated
# grid-search over a parameter grid.
# KFold splits dataset into k consecutive folds (without shuffling by default)

inner_cv = KFold(n_splits=5, shuffle=True, random_state = 42) # inner cross-validation folds
outer_cv = KFold(n_splits=5, shuffle=True, random_state = 42) # outer cross-validation folds

"""### Parameter Tuning"""

############################## Decision Tree Parameter Tuning ##############################

# Create max_depth list for later
max_depth = [None, 1, 3, 5, 7, 10, 12, 15, 17, 20, 23, 25, 30, 35, 40]

gs_dt2 = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),
                  param_grid=[{'max_depth': max_depth,
                               'criterion':['gini','entropy'],
                               'min_samples_leaf':[1,2,3,4,5],
                               'min_samples_split':[2,3,4,5]}],
                  scoring='accuracy',
                  cv=inner_cv,
                  n_jobs=-1)

gs_dt2 = gs_dt2.fit(X_smote,y_smote)
print("\n Parameter Tuning #3")
print("Non-nested CV Accuracy: ", gs_dt2.best_score_)
print("Optimal Parameter: ", gs_dt2.best_params_)
print("Optimal Estimator: ", gs_dt2.best_estimator_)
nested_score_gs_dt2 = cross_val_score(gs_dt2, X=X_test0, y=y_test0, cv=outer_cv)
print("Nested CV Accuracy: ",nested_score_gs_dt2.mean(), " +/- ", nested_score_gs_dt2.std())

"""## KNN

### Setup

#### Normalize Data
"""

#Normalize Data
pipe = Pipeline([
        ('sc', StandardScaler()),
        ('knn', KNeighborsClassifier(p=2,
                                     metric='minkowski'))
      ])

"""### k-NN Parameter Tuning"""

################################### kNN Parameter Tuning ###################################

#Parameters to optimize
params = {
        'knn__n_neighbors': list(range(1, 30))
    }

#Parameters to optimize:  k for number of nearest neighbors AND type of distance

params = {
        'knn__n_neighbors': list(range(1, 30)),
        'knn__weights': ['uniform', 'distance']
    }

gs_knn2 = GridSearchCV(estimator=pipe,
                  param_grid=params,
                  scoring='accuracy',
                  cv=inner_cv,
                  n_jobs=4)

gs_knn2 = gs_knn2.fit(X_smote,y_smote.values.ravel())
print("\n Parameter Tuning #7")
print("Non-nested CV Accuracy: ", gs_knn2.best_score_)
print("Optimal Parameter: ", gs_knn2.best_params_)
print("Optimal Estimator: ", gs_knn2.best_estimator_) # Estimator that was chosen by the search, i.e. estimator which gave highest score
nested_score_gs_knn2 = cross_val_score(gs_knn2, X=X_test0, y=y_test0.values.ravel(), cv=outer_cv)
print("Nested CV Accuracy: ",nested_score_gs_knn2.mean(), " +/- ", nested_score_gs_knn2.std())

"""## Logistic Regression

### Parameter Tuning
"""

############################ Logistic Regression Parameter Tuning ############################
#To ignore the convergence warnings
from  warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

# Choosing C parameter for Logistic Regression AND type of penalty (ie., l1 vs l2)
# See other parameters here http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
gs_lr2 = GridSearchCV(estimator=LogisticRegression(random_state=42, solver='liblinear'),
                  param_grid=[{'C': [ 0.00001, 0.0001, 0.001, 0.01, 0.1 ,1 ,10 ,100, 1000, 10000, 100000, 1000000, 10000000],
                              'penalty':['l1','l2']}],
                  scoring='accuracy',
                  cv=inner_cv)

gs_lr2 = gs_lr2.fit(X_smote,y_smote.values.ravel())
print("\n Parameter Tuning #5")
print("Non-nested CV Accuracy: ", gs_lr2.best_score_)
print("Optimal Parameter: ", gs_lr2.best_params_)
print("Optimal Estimator: ", gs_lr2.best_estimator_)

nested_score_gs_lr2 = cross_val_score(gs_lr2, X=X_test0, y=y_test0.values.ravel(), cv=outer_cv)
print("Nested CV Accuracy:",nested_score_gs_lr2.mean(), " +/- ", nested_score_gs_lr2.std())

"""# Evaluation

gs_dt2

DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_split=3,
                       random_state=42)

gs_knn2

Pipeline(steps=[('sc', StandardScaler()),
                ('knn', KNeighborsClassifier(n_neighbors=2))])

gs_lr2  

LogisticRegression(C=1, random_state=42, solver='liblinear')
"""

# Define the plot_confusion_matrix function
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize the confusion matrix
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if i==j else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

from sklearn.metrics import confusion_matrix
import itertools
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score

y_pred1 = gs_dt2.predict(X_test0)
cnf_matrix1 = confusion_matrix(y_test0, y_pred1)

# Determine the way floating point numbers are displayed
np.set_printoptions(precision=2)

print("f1: ",f1_score(y_test0, y_pred1, average='weighted'))
print("recall: ",recall_score(y_test0, y_pred1, average='weighted'))

# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix1,
                      classes=["No", "Yes"],  # Replace with your actual class names
                      title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix1,
                      classes=["No", "Yes"],  # Replace with your actual class names
                      normalize=True,
                      title='Normalized confusion matrix')

plt.show()

from sklearn.metrics import confusion_matrix
import itertools
y_pred2 = gs_knn2.predict(X_test0)
cnf_matrix2 = confusion_matrix(y_test0, y_pred2)

# Determine the way floating point numbers are displayed
np.set_printoptions(precision=2)


print("f1: ",f1_score(y_test0, y_pred2, average='weighted'))
print("recall: ",recall_score(y_test0, y_pred2, average='weighted'))

# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix2,
                      classes=["No", "Yes"],
                      title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix2,
                      classes=["No", "Yes"],
                      normalize=True,
                      title='Normalized confusion matrix')

plt.show()

y_pred3 = gs_lr2.predict(X_test0)
cnf_matrix3 = confusion_matrix(y_test0, y_pred3)

# Determine the way floating point numbers are displayed
np.set_printoptions(precision=2)


print("f1: ",f1_score(y_test0, y_pred3, average='weighted'))
print("recall: ",recall_score(y_test0, y_pred3, average='weighted'))

# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix3,
                      classes=["No", "Yes"],
                      title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix3,
                      classes=["No", "Yes"],
                      normalize=True,
                      title='Normalized confusion matrix')

plt.show()